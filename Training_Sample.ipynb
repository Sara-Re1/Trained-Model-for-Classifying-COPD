{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95yJHHsTcVmX"
      },
      "outputs": [],
      "source": [
        "#Resizing input data\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def resize_data(data):\n",
        "    initial_size_x = data.shape[1]\n",
        "    initial_size_y = data.shape[2]\n",
        "    initial_size_z = data.shape[3]\n",
        "\n",
        "    new_size_x = 16\n",
        "    new_size_y = 128\n",
        "    new_size_z = 128\n",
        "    delta_x = initial_size_x / new_size_x\n",
        "    delta_y = initial_size_y / new_size_y\n",
        "    delta_z = initial_size_z / new_size_z\n",
        "\n",
        "    new_data = np.zeros((data.shape[0], new_size_x, new_size_y, new_size_z))\n",
        "\n",
        "    for i, x, y, z in itertools.product(range(data.shape[0]),\n",
        "                                       range(new_size_x),\n",
        "                                       range(new_size_y),\n",
        "                                       range(new_size_z)):\n",
        "        new_data[i][x][y][z] = data[i, int(x * delta_x), int(y * delta_y), int(z * delta_z)]\n",
        "\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCjKpvnIKeJo"
      },
      "outputs": [],
      "source": [
        "#Importing Train Data\n",
        "trainTargets = pd.read_excel(....)\n",
        "trainTargets = trainTargets.values.tolist()\n",
        "trainTargets=np.array(trainTargets)\n",
        "traindata=np.load('.....npy')\n",
        "\n",
        "# Resize Train Data\n",
        "traindata= resize_data(traindata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9_dpwQPvui7"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "data=traindata\n",
        "labels=trainTargets\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define lists to store the results\n",
        "val_AUC=[]\n",
        "val2_loss=[]\n",
        "val_acc=[]\n",
        "val_recal=[]\n",
        "val2_precision=[]\n",
        "\n",
        "train_AUC=[]\n",
        "train2_loss=[]\n",
        "train_acc=[]\n",
        "train_recal=[]\n",
        "train2_precision=[]\n",
        "\n",
        "# Define early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "num_classes = 2\n",
        "\n",
        "for train_index, val_index in skf.split(data, labels):\n",
        "    # Split the data into training and validation sets\n",
        "    x_train, x_val = data[train_index], data[val_index]\n",
        "    y_train, y_val = labels[train_index], labels[val_index]\n",
        "\n",
        "\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "    x_val = np.expand_dims(x_val, axis=-1)\n",
        "\n",
        "    # Calculate weights using sklearn for imbalanced data\n",
        "    yy = y_train.flatten()\n",
        "\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(yy), y=yy)\n",
        "    class_weights_dict = dict(enumerate(class_weights))\n",
        "    print(class_weights_dict)\n",
        "\n",
        "    y_train2 = to_categorical(y_train, num_classes)\n",
        "    y_val2 = to_categorical(y_val, num_classes)\n",
        "\n",
        "    initial_learning_rate = 0.00001\n",
        "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate, decay_steps=10000, decay_rate=0.50, staircase=True\n",
        "    )\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "        metrics=[\n",
        "            tf.keras.metrics.CategoricalAccuracy(),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recal'),\n",
        "            tf.keras.metrics.AUC(name='auc')])\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    batch_size =50\n",
        "    epochs =20\n",
        "    print(x_train.shape)\n",
        "    history = model.fit(x_train, y_train2, batch_size=batch_size, epochs=epochs,\n",
        "                    validation_data=(x_val, y_val2), class_weight=class_weights_dict,\n",
        "                    callbacks=[early_stop])\n",
        "\n",
        "    # Access the training metrics from the history object\n",
        "    train_acc1 = history.history['categorical_accuracy']\n",
        "    train_loss1 = history.history['loss']\n",
        "    train_precision1 = history.history['precision']\n",
        "    train_recal1 = history.history['recal']\n",
        "    train_AUC1 = history.history['auc']\n",
        "\n",
        "    # Access the validation metrics from the history object\n",
        "    val_acc1 = history.history['val_categorical_accuracy']\n",
        "    val_loss1 = history.history['val_loss']\n",
        "    val_precision1 = history.history['val_precision']\n",
        "    val_recal1 = history.history['val_recal']\n",
        "    val_AUC1 = history.history['val_auc']\n",
        "\n",
        "    # Plotting the training and validation accuracy over epochs\n",
        "    plt.plot(train_acc1, label='Training Accuracy')\n",
        "    plt.plot(val_acc1, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # Plotting the training and validation accuracy over epochs\n",
        "    plt.plot(train_loss1, label='Training Loss')\n",
        "    plt.plot(val_loss1, label='Validation Loss')\n",
        "    plt.title('Training and Validation Accuracy Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # Plotting AUC\n",
        "    if 'auc' in history.history:\n",
        "        plt.plot(history.history['auc'])\n",
        "        plt.plot(history.history['val_auc'])\n",
        "        plt.title('Model AUC')\n",
        "        plt.ylabel('AUC')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"AUC not found in history. Make sure 'AUC' is included as a metric in model compilation.\")\n",
        "\n",
        "    # Evaluate the model on the training and validation sets\n",
        "    a=model.evaluate(x_train, y_train2, verbose=1)\n",
        "    train_loss, train_accuracy, train_precision, train_recall, train_auc= model.evaluate(x_train, y_train2, verbose=1)\n",
        "    val_loss, val_accuracy, val_precision, val_recall, val_auc= model.evaluate(x_val, y_val2, verbose=1)\n",
        "    print(a)\n",
        "\n",
        "    # Append the results to the lists\n",
        "    train_acc.append(train_accuracy)\n",
        "    train_AUC.append(train_auc)\n",
        "    train2_loss.append(train_loss)\n",
        "    train_recal.append(train_recall)\n",
        "    train2_precision.append(train_precision)\n",
        "\n",
        "    val_acc.append(val_accuracy)\n",
        "    val_AUC.append(val_auc)\n",
        "    val2_loss.append(val_loss)\n",
        "    val_recal.append(val_recall)\n",
        "    val2_precision.append(val_precision)\n",
        "\n",
        "loss_max=max(val2_loss)\n",
        "loss_ave=np.mean(val_loss)\n",
        "loss_std=np.std(val_loss)\n",
        "\n",
        "acc_max=max(val_acc)\n",
        "acc_ave=np.mean(val_acc)\n",
        "acc_std=np.std(val_acc)\n",
        "\n",
        "prec_max=max(val2_precision)\n",
        "prec_ave=np.mean(val2_precision)\n",
        "prec_std=np.std(val2_precision)\n",
        "\n",
        "recal_max=max(val_recal)\n",
        "recal_ave=np.mean(val_recal)\n",
        "recal_std=np.std(val_recal)\n",
        "\n",
        "AUC_max=max(val_AUC)\n",
        "AUC_ave=np.mean(val_AUC)\n",
        "AUC_std=np.std(val_AUC)\n",
        "\n",
        "\n",
        "print('val_loss=', loss_ave,\"+-\", loss_std, \"MAX=\", loss_max)\n",
        "print('val_acc=', acc_ave,\"+-\", acc_std, \"MAX=\", acc_max)\n",
        "print('val_AUC=', AUC_ave,\"+-\", AUC_std, \"MAX=\", AUC_max)\n",
        "print('val_precision=', prec_ave,\"+-\", prec_std, \"MAX=\", prec_max)\n",
        "print('val_recal=', recal_ave,\"+-\", recal_std, \"MAX=\", recal_max)\n",
        "\n",
        "train_loss_max=max(train2_loss)\n",
        "train_loss_ave=np.mean(train2_loss)\n",
        "train_loss_std=np.std(train2_loss)\n",
        "\n",
        "train_acc_max=max(train_acc)\n",
        "train_acc_ave=np.mean(train_acc)\n",
        "train_acc_std=np.std(train_acc)\n",
        "\n",
        "train_prec_max=max(train2_precision)\n",
        "train_prec_ave=np.mean(train2_precision)\n",
        "train_prec_std=np.std(train2_precision)\n",
        "\n",
        "train_recal_max=max(train_recal)\n",
        "train_recal_ave=np.mean(train_recal)\n",
        "train_recal_std=np.std(train_recal)\n",
        "\n",
        "train_AUC_max=max(train_AUC)\n",
        "train_AUC_ave=np.mean(train_AUC)\n",
        "train_AUC_std=np.std(train_AUC)\n",
        "\n",
        "\n",
        "print('train_loss=', train_loss_ave,\"+-\", train_loss_std, \"MAX=\", train_loss_max)\n",
        "print('train_acc=', train_acc_ave,\"+-\", train_acc_std, \"MAX=\", train_acc_max)\n",
        "print('train_AUC=', train_AUC_ave,\"+-\", train_AUC_std, \"MAX=\", train_AUC_max)\n",
        "print('train_precision=', train_prec_ave,\"+-\", train_prec_std, \"MAX=\", train_prec_max)\n",
        "print('train_recal=', train_recal_ave,\"+-\", train_recal_std, \"MAX=\", train_recal_max)\n",
        "\n",
        "\n",
        "# Calculate the average training and validation accuracies across folds\n",
        "avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "print(\"Average training accuracy: \", avg_train_acc)\n",
        "\n",
        "################################\n",
        "c=(time.time() - start_time)/60\n",
        "print('training time (min)=', c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y72tmWO2zN67"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml h5py\n",
        "# save model\n",
        "model.save('......h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCmASF8IBSOH"
      },
      "outputs": [],
      "source": [
        "### Importing Test Data ####\n",
        "\n",
        "testTargets = pd.read_excel(....)\n",
        "testTargets = testTargets.values.tolist()\n",
        "testTargets=np.array(testTargets)\n",
        "testdata=np.load(r'.....npy')\n",
        "# Resize test data\n",
        "testdata= resize_data(testdata)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Prepare test data\n",
        "x_test = testdata  # Assuming testdata is in shape (num_samples, 256, 256, 15)\n",
        "y_test = testTargets  # Assuming testTargets is in shape (num_samples,)\n",
        "x_test = np.expand_dims(x_test, axis=-1)  # Add channel dimension to match model input shape (num_samples, 256, 256, 15, 1)\n",
        "y_test2 = to_categorical(y_test, num_classes=2)  # Convert y_test to one-hot encoding for model evaluation\n",
        "\n",
        "# Evaluate model u\n",
        "TEST = model.evaluate(x_test, y_test2, batch_size=5, verbose=1)\n",
        "test_loss, test_accuracy, test_precision, test_recall, test_auc = TEST\n",
        "\n",
        "\n",
        "# Predict probabilities for calculating additional metrics\n",
        "y_pred_probs = model.predict(x_test)\n",
        "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Flatten y_test only if it's 2D\n",
        "# y_test = np.argmax(y_test2, axis=1)\n",
        "\n",
        "# Calculate the Brier Score\n",
        "brsc = brier_score_loss(y_test, y_pred_probs[:, 1])\n",
        "###########################\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Predict labels and probabilities\n",
        "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_labels)\n",
        "tn, fp, fn, tp = cm.ravel()  # Flatten confusion matrix\n",
        "\n",
        "# Calculate Precision, Recall, F1 Score, and Specificity\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall) if (test_precision + test_recall) > 0 else 0\n",
        "\n",
        "# Function to calculate 95% confidence intervals\n",
        "def confidence_interval(metric, n, z=1.96):\n",
        "    return z * np.sqrt((metric * (1 - metric)) / n)\n",
        "\n",
        "# Calculate confidence intervals\n",
        "acc_ci = confidence_interval(test_accuracy, len(y_test))\n",
        "pre_ci = confidence_interval(test_precision, len(y_test))\n",
        "rec_ci = confidence_interval(test_recall, len(y_test))\n",
        "auc_ci = confidence_interval(test_auc, len(y_test))\n",
        "f1_ci = confidence_interval(f1, len(y_test))\n",
        "spec_ci = confidence_interval(specificity, len(y_test))\n",
        "\n",
        "# Print results\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ± {acc_ci:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f} ± {pre_ci:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f} ± {rec_ci:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f} ± {f1_ci:.4f}\")\n",
        "print(f\"Test Specificity: {specificity:.4f} ± {spec_ci:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f} ± {auc_ci:.4f}\")\n",
        "print(f\"Brier Score: {brsc:.4f}\")\n"
      ],
      "metadata": {
        "id": "33t6s9_406Ug"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}